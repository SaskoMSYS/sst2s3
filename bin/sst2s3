#!/usr/bin/env ruby

require 'rubygems'

require 'yaml'
require 'etc'

$:.unshift File.join(File.dirname(__FILE__), '../lib')

conffile = '/etc/cassandra/cassandra.yaml'

require 'sst2s3'

opts = Trollop::options do
  version "Version #{SST::version}"
  opt :s3_key, "S3 Key", :type => :string
  opt :s3_secret, "S3 Secret", :type => :string
  opt :s3_bucket, "S3 Bucket+Path (eg: bucket_name/path/to/storedir)", :type => :string
  opt :dir, "Data Directory", :type => :string
  opt :tmpdir, "Directory for tempfiles", :type => :string, :default => "/tmp"
  opt :verbose, "Print checkpoints when archiving"
  opt :all, "Backup all keyspaces"
end

[:s3_key, :s3_secret, :s3_bucket].each do |k|
  Trollop::die k, "Required option" unless opts[k]
end

bparse = opts[:s3_bucket].split('/')
unless bparse.length >= 2
  Trollop::die :s3_bucket, "Invalid format: bucket_name/path/to/storedir"
end

bucket_name = bparse[0]
bucket_path = "/" + bparse[1, bparse.length].join("/")

if File.exist?(conffile)
  begin
    yf = File.open(conffile) {|f| YAML::load(f)}
    opts[:dir] = yf['data_file_directories'][0]
  rescue
  end
end

unless opts[:dir] && File.exist?(opts[:dir])
  Trollop::die(:dir, "Directory not set or invalid")
end

keyspaces = ARGV
unless keyspaces.length > 0 || opts[:all]
  Trollop::die "Must specify a keyspace or --all"
end

if opts[:all]
  keyspaces = Dir.glob(File.join(opts[:dir], '*')).collect{|f| File.basename(f)}
  if keyspaces.length == 0
    $stderr.puts "Error: No keyspaces found at #{opts[:dir]}"
    exit 1
  end
end

AWS::S3::Base.establish_connection!(
    :access_key_id     => opts[:s3_key],
    :secret_access_key => opts[:s3_secret]
  )

begin
  bucket = AWS::S3::Bucket.find(bucket_name)
rescue
  $stderr.puts "Error: Unable to locate bucket: #{bucket_name}"
  exit 1
end

pkglist = {:keyspaces => {}, :version => SST::plist_version}
keyspaces.each do |ks|
  ksdir = File.join(opts[:dir], ks)
  files = Dir.glob(File.join(ksdir, '*')).
    reject{|f| File.directory?(f)}.collect{|f| File.basename(f)}
  pkglist[:keyspaces][ks] = {:files => {}}
  Dir.chdir(ksdir) do
    files.each do |f|
      sha1sum = %x{sha1sum #{f} | cut -d ' ' -f 1}.chomp
      unless $?.exited? && $?.exitstatus == 0
        $stderr.puts "Failed to calculate sha1sum for #{f}"
        exit 1
      end

      stat = File.stat(f)

      fmeta = {
        :sha1 => sha1sum,
        :size => stat.size
      }

      pwuid = Etc.getpwuid(stat.uid) rescue nil
      fmeta[:owner] = pwuid.name if pwuid
      grgid = Etc.getgrgid(stat.gid) rescue nil
      fmeta[:group] = grgid.name if grgid

      pkglist[:keyspaces][ks][:files][f] = fmeta
    end

    puts "> Archiving keyspace #{ks}"

    archive = "#{bucket_path}/#{ks}.tar.gz"
    tmp = "#{opts[:tmpdir]}/archive_#{rand 99999999}.tar.gz"
    checkpoint = opts[:verbose] ? " --checkpoint" : ""
    r = system("tar#{checkpoint} -zcf #{tmp} #{files.join(" ")}")
    unless r
      $stderr.puts "Error: Unable to compress #{ks} sstables"
      exit 1
    end

    #
    # XXX: Stream tar output directly to upload. S3Object.store()
    # requires the size though, http limitation?
    #
    puts "> Uploading keyspace #{ks}"
    AWS::S3::S3Object.store(archive, File.open(tmp), bucket_name)
    FileUtils::rm(tmp)
    pkglist[:keyspaces][ks][:archive] = archive
  end
end

# Upload pkglist
#
AWS::S3::S3Object.store("#{bucket_path}/plist.json",
                        pkglist.to_json,
                        bucket_name,
                        :content_type => 'application/json')
